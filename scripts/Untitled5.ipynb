{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRMY15-3VWeE",
    "outputId": "d7b4b8c3-36f3-4990-ba31-051393e9e236"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post11.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m275.7/275.7 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.9.26)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n",
      "Building wheels for collected packages: sklearn, lime\n",
      "  Building wheel for sklearn (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post11-py3-none-any.whl size=2959 sha256=d55116b7aa95e7345ffb1aa4b39a627f910fb512b9f2d8c4c0e188d0d4a116c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/aa/9c/60/f67813603a52fc35057868f1aba0003cc75b72583dcaa2c341\n",
      "  Building wheel for lime (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=3bba8bee4ff826d04afbdae231451a38acb47a59b634ec62810406b54f6f9c9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
      "Successfully built sklearn lime\n",
      "Installing collected packages: sklearn, lime\n",
      "Successfully installed lime-0.2.0.1 sklearn-0.0.post11\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib numpy pandas nltk sklearn nltk tqdm matplotlib lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "eOzA94yiVXkL",
    "outputId": "796ccf23-021c-418d-a18c-3101754f35fd",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:57:41.467704400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "Drop empty data...\n",
      "Making a lower case\n",
      "Removing punctuation and stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23640/23640 [02:50<00:00, 138.81it/s]\n",
      "100%|██████████| 23640/23640 [03:44<00:00, 105.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting...\n",
      "First half...\n",
      "Error in callback <function _draw_all_if_interactive at 0x000001E3B58B7130> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import ShuffleSplit, learning_curve\n",
    "from sklearn.svm import SVC, SVR\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "# import numpy as np\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "tqdm.pandas()\n",
    "# Load and preprocess the data while removing rows with empty cells\n",
    "print(\"Reading...\")\n",
    "data = pd.read_csv('./Bases/result.csv', na_values=[''])\n",
    "\n",
    "print(\"Drop empty data...\")\n",
    "data.dropna(subset=['text'], inplace=True)  # Remove rows with empty 'text' cells\n",
    "print(\"Making a lower case\")\n",
    "data['text'] = data['text'].str.lower()\n",
    "print(\"Removing punctuation and stemming...\")\n",
    "stemmer = LancasterStemmer()\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "data['text'] = data['text'].progress_apply(remove_stopwords)\n",
    "# data['text'] = data['name'] + ' ' + data['text']\n",
    "data['text'] = data['text'].progress_apply(lambda x: ' '.join([stemmer.stem(word) for word in re.sub(r'[.,:;\"\\'!?\\-’“%()]', '', remove_links(x)).split()]))\n",
    "data_x = data['text']\n",
    "data_y = data['target_numeric']\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)\n",
    "clf = SVC(kernel='linear', probability=True, cache_size=200, max_iter=10000)\n",
    "# Split the data using ShuffleSplit\n",
    "print(\"Splitting...\")\n",
    "\n",
    "spl = ShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in spl.split(data_x, data_y):\n",
    "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "\n",
    "    x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "    x_test_vectorized = vectorizer.transform(x_test)\n",
    "# fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "# plot_learning_curve(model, \"Learning curve\", x_train, y_train, axes=axes, ylim=(0.0, 1.01),\n",
    "#                     cv=5, n_jobs=4)\n",
    "# plt.show()\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "X=x_train_vectorized\n",
    "y=y_train\n",
    "print(\"First half...\")\n",
    "title = \"Learning Curves SVR(C=1.0, epsilon=0.2)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "\n",
    "estimator = SVR(C=1.0, epsilon=0.2)\n",
    "plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "print(\"Second half...\")\n",
    "title = r\"Learning Curves SVC(kernel='linear')\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "estimator = SVC(kernel='linear', probability=True, cache_size=200, max_iter=1000)\n",
    "plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
