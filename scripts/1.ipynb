{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T17:41:52.987465200Z",
     "start_time": "2023-11-27T17:39:52.609857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "!pip install lime sklearn pandas numpy git+https://github.com/Desklop/Uk_Stemmer tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T17:45:33.465275900Z",
     "start_time": "2023-11-27T17:41:53.003462600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n",
      "0        us aid foreign countries consist military assi...\n",
      "1        recent appearance new hampshire democratic pre...\n",
      "2        us sen ted cruz wants people know texas — cali...\n",
      "3        instagram post attributes dystopian statement ...\n",
      "4        election day glitch eastern pennsylvania socia...\n",
      "                               ...                        \n",
      "23635    richardson led successful effort raise teacher...\n",
      "23636    indeed economists found top tier americans ear...\n",
      "23637    debate south carolina tancredo said could matc...\n",
      "23638    certainly risky claim fatherhood major policy ...\n",
      "23639    milwaukee parental choice program established ...\n",
      "Name: text, Length: 23640, dtype: object\n",
      "Stemming dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import lime\n",
    "import sklearn\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import nltk\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#data = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1TSpzR1juNcdcL_08TbUvKTpsfdMQPohdhVqO5rmPGEY/gviz/tq?tqx=out:csv&sheet=Mirniy_Vovchok\")\n",
    "print('Loading dataset')\n",
    "data = pd.read_csv('./Bases/result.csv')\n",
    "\n",
    "a1 = []\n",
    "f1 = []\n",
    "class_names = ['mostly true',  'mostly false']\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "print('Preprocessing dataset')\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)\n",
    "data['text'] = data['text'].str.lower()\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "data['text'] = data['text'].apply(remove_links)\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[.,:;\"\\'!?\\-’“%()]', '', x).split())\n",
    "def remove_stopwords(text):\n",
    "    # word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "data['text'] = data['text'].apply(remove_stopwords)\n",
    "data_x = data['text']\n",
    "\n",
    "\n",
    "print(data['text'])\n",
    "\n",
    "print('Stemming dataset')\n",
    "data_xx=[]\n",
    "for v in data_x:\n",
    "    from uk_stemmer import UkStemmer\n",
    "\n",
    "    stemmer = nltk.LancasterStemmer()\n",
    "\n",
    "    test_string=v\n",
    "    test_string.replace('we rated that attribution', '')\n",
    "    test_string.replace('we rated a claim that gates', '')\n",
    "    test_string.replace('we rated that claim', '')\n",
    "    test_string.replace('we rated that', '')\n",
    "    test_string.replace('we rated a similar statement', '')\n",
    "    test_string.replace('we rated his statement', '')\n",
    "    test_string.replace('we rated the statement', '')\n",
    "    test_string.replace('we rated it', '')\n",
    "    test_string.replace('mostly true', '')\n",
    "    test_string.replace('we rated', '')\n",
    "    test_string.replace('false', '')\n",
    "    test_string.replace('pants on fire', '')\n",
    "    test_string.replace('mostly false', '')\n",
    "    test_string.replace('half true', '')\n",
    "    test_string.replace('true', '')\n",
    "    test_string.replace('barely true', '')\n",
    "    test_string.replace('mostly true', '')\n",
    "    test_string.replace('full flop', '')\n",
    "    test_string.replace('no flip', '')\n",
    "    test_string.replace('half flip', '')\n",
    "    \n",
    "    # get_test_string = prepare_test_string.replace('європейська', '')\n",
    "    # get_test_string = prepare_test_string.replace('європейська правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('економічна', '')\n",
    "    # get_test_string = prepare_test_string.replace('економічна правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('цензор', '')\n",
    "    # get_test_string = prepare_test_string.replace('нет', '')\n",
    "    # get_test_string = prepare_test_string.replace('цензор.нет', '')\n",
    "    words = re.split(r'(\\W)', test_string)\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = stemmer.stem(words[i])\n",
    "\n",
    "    stem_test_string = ''.join(words)\n",
    "    #print('Source: %s\\nStemmed: %s' % (test_string, stem_test_string))\n",
    "\n",
    "    data_xx.append(stem_test_string)\n",
    "\n",
    "\n",
    "#data_x=np.array(data_xx)\n",
    "data_x = np.array(data_xx)\n",
    "\n",
    "data_y = data['target_numeric']\n",
    "\n",
    "kf = ShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "\n",
    "print('Splitting dataset')\n",
    "\n",
    "for train_index, test_index in kf.split(data_x,data_y):\n",
    "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "    \n",
    "    x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "    x_test_vectorized = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = SVC(kernel='linear', probability=True, cache_size=200, max_iter=10000)\n",
    "    print('Training dataset')\n",
    "    clf.fit(x_train_vectorized, y_train)\n",
    "    y_pred = clf.predict(x_test_vectorized)\n",
    "    print('Evaluating dataset')\n",
    "    score_accuracy = accuracy_score(y_test, y_pred)\n",
    "    score_f1 = sklearn.metrics.f1_score(y_test, y_pred, average='binary')\n",
    "    score_precision = precision_score(y_test, y_pred, average='binary')\n",
    "    score_recall = recall_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    a1.append(score_accuracy)\n",
    "    f1.append(score_f1)\n",
    "\n",
    "    print('F1 Score = ', score_f1)\n",
    "    print('Accuracy Score = ', score_accuracy)\n",
    "    print('Precision Score = ', score_precision)\n",
    "    print('Recall Score = ', score_recall)\n",
    "\n",
    "    print('Explaining dataset')\n",
    "    c = make_pipeline(vectorizer, clf)\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    print(test_index)\n",
    "    idx = 0\n",
    "    \n",
    "    exp = explainer.explain_instance(x_test[idx], c.predict_proba, num_features=400)\n",
    "    \n",
    "    print('Document id: %d' % idx)\n",
    "    print('Probability(Sciense) =', c.predict_proba([x_test[idx]])[0,1])\n",
    "    #print('True class: %s' % class_names[y_test[idx]])\n",
    "    print('Original prediction:', clf.predict_proba(x_test_vectorized[idx])[0,1])\n",
    "    \n",
    "    tmp = x_test_vectorized[idx].copy()\n",
    "    \n",
    "    print('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\n",
    "    print('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(x_test_vectorized[idx])[0,1])\n",
    "\n",
    "    %matplotlib inline\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    fig.set_size_inches(18.5, 40)\n",
    "    exp.save_to_file(\"1.html\")\n",
    "    exp.show_in_notebook(text=True)\n",
    "\n",
    "    exp.as_list()\n",
    "print(\"acc\",np.average(a1))\n",
    "print(\"f1\",np.average(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de9648246f10b14b7317fc3bc9f2a1853aab7a7038751f28566d3e3a7ef3745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
