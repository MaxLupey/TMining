{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n",
      "0        us aid foreign countries consist military assi...\n",
      "1        recent appearance new hampshire democratic pre...\n",
      "2        us sen ted cruz wants people know texas — cali...\n",
      "3        instagram post attributes dystopian statement ...\n",
      "4        election day glitch eastern pennsylvania socia...\n",
      "                               ...                        \n",
      "23635    richardson led successful effort raise teacher...\n",
      "23636    indeed economists found top tier americans ear...\n",
      "23637    debate south carolina tancredo said could matc...\n",
      "23638    certainly risky claim fatherhood major policy ...\n",
      "23639    milwaukee parental choice program established ...\n",
      "Name: text, Length: 23640, dtype: object\n",
      "Stemming dataset\n",
      "Splitting dataset\n",
      "Training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dataset\n",
      "F1 Score =  0.8251273344651954\n",
      "Accuracy Score =  0.7821489001692047\n",
      "Precision Score =  0.7655954631379962\n",
      "Recall Score =  0.894698085419735\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#data = pd.read_csv(\"https://docs.google.com/spreadsheets/d/1TSpzR1juNcdcL_08TbUvKTpsfdMQPohdhVqO5rmPGEY/gviz/tq?tqx=out:csv&sheet=Mirniy_Vovchok\")\n",
    "print('Loading dataset')\n",
    "data = pd.read_csv('../Bases/result.csv')\n",
    "\n",
    "a1 = []\n",
    "f1 = []\n",
    "class_names = ['mostly true',  'mostly false']\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "print('Preprocessing dataset')\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)\n",
    "data['text'] = data['text'].str.lower()\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "data['text'] = data['text'].apply(remove_links)\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[.,:;\"\\'!?\\-’“%()]', '', x).split())\n",
    "def remove_stopwords(text):\n",
    "    # word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "data['text'] = data['text'].apply(remove_stopwords)\n",
    "data_x = data['text']\n",
    "\n",
    "\n",
    "print(data['text'])\n",
    "\n",
    "print('Stemming dataset')\n",
    "data_xx=[]\n",
    "for v in data_x:\n",
    "\n",
    "    stemmer = nltk.LancasterStemmer()\n",
    "\n",
    "    test_string=v\n",
    "    test_string.replace('we rated that attribution', '')\n",
    "    test_string.replace('we rated a claim that gates', '')\n",
    "    test_string.replace('we rated that claim', '')\n",
    "    test_string.replace('we rated that', '')\n",
    "    test_string.replace('we rated a similar statement', '')\n",
    "    test_string.replace('we rated his statement', '')\n",
    "    test_string.replace('we rated the statement', '')\n",
    "    test_string.replace('we rated it', '')\n",
    "    test_string.replace('mostly true', '')\n",
    "    test_string.replace('we rated', '')\n",
    "    test_string.replace('false', '')\n",
    "    test_string.replace('pants on fire', '')\n",
    "    test_string.replace('mostly false', '')\n",
    "    test_string.replace('half true', '')\n",
    "    test_string.replace('true', '')\n",
    "    test_string.replace('barely true', '')\n",
    "    test_string.replace('mostly true', '')\n",
    "    test_string.replace('full flop', '')\n",
    "    test_string.replace('no flip', '')\n",
    "    test_string.replace('half flip', '')\n",
    "    \n",
    "    # get_test_string = prepare_test_string.replace('європейська', '')\n",
    "    # get_test_string = prepare_test_string.replace('європейська правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('економічна', '')\n",
    "    # get_test_string = prepare_test_string.replace('економічна правда', '')\n",
    "    # get_test_string = prepare_test_string.replace('цензор', '')\n",
    "    # get_test_string = prepare_test_string.replace('нет', '')\n",
    "    # get_test_string = prepare_test_string.replace('цензор.нет', '')\n",
    "    # words = re.split(r'(\\W)', get_test_string)\n",
    "    words = [word for word in str(v) if word != '']\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = stemmer.stem(words[i])\n",
    "\n",
    "    stem_test_string = ''.join(words)\n",
    "    #print('Source: %s\\nStemmed: %s' % (test_string, stem_test_string))\n",
    "\n",
    "    data_xx.append(stem_test_string)\n",
    "\n",
    "\n",
    "#data_x=np.array(data_xx)\n",
    "data_x = np.array(data_xx)\n",
    "\n",
    "data_y = data['target_numeric']\n",
    "\n",
    "kf = ShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "\n",
    "print('Splitting dataset')\n",
    "\n",
    "for train_index, test_index in kf.split(data_x,data_y):\n",
    "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "    \n",
    "    x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "    x_test_vectorized = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = SVR(kernel='linear', max_iter=10000)\n",
    "    print('Training dataset')\n",
    "    clf.fit(x_train_vectorized, y_train)\n",
    "    y_pred = clf.predict(x_test_vectorized)\n",
    "    predictions1 = []\n",
    "    for a in y_pred:\n",
    "        predictions1.append(bool(round(a)))\n",
    "    print('Evaluating dataset')\n",
    "    score_accuracy = accuracy_score(y_test, predictions1)\n",
    "    score_f1 = sklearn.metrics.f1_score(y_test, predictions1, average='binary')\n",
    "    score_precision = precision_score(y_test, predictions1, average='binary')\n",
    "    score_recall = recall_score(y_test, predictions1, average='binary')\n",
    "    \n",
    "    a1.append(score_accuracy)\n",
    "    f1.append(score_f1)\n",
    "\n",
    "    print('F1 Score = ', score_f1)\n",
    "    print('Accuracy Score = ', score_accuracy)\n",
    "    print('Precision Score = ', score_precision)\n",
    "    print('Recall Score = ', score_recall)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T18:48:59.373872100Z",
     "start_time": "2023-11-24T18:29:10.043036700Z"
    }
   },
   "id": "d033584d0119e1d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3635019bca4ad743"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
