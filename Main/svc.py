# -*- coding: utf-8 -*-
"""SVC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTPemKeIP2G6IbKSgYPOtzV1S8ek5rzn
"""

# pip install lime sklearn pandas numpy git+https://github.com/Desklop/Uk_Stemmer tensorflow==2.0.0



from __future__ import print_function # from print print_functions to print_function
import lime
import sklearn
import pandas as pd
import numpy as np
import sklearn
import sklearn.ensemble
import sklearn.metrics
import sklearn.feature_extraction.text
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
from sklearn.model_selection import KFold
from sklearn.model_selection import ShuffleSplit
from sklearn.svm import SVR
from sklearn.svm import SVC
from lime import lime_text
from sklearn.pipeline import make_pipeline
from lime.lime_text import LimeTextExplainer
from nltk import word_tokenize, pos_tag, ne_chunk

#data = pd.read_csv("dataset.csv") # /content/drive/MyDrive/news-parser/Articles.csv
data = pd.read_csv("result.csv")
data.dropna(subset=['text'], inplace=True)



a1 = []
f1 = []
class_names = ['Pravda', 'Politeka']

vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)

data_x = data['text']
print(data['text'])

data_xx=[]
for v in data_x:
    import re
    from uk_stemmer import UkStemmer
    stemmer = UkStemmer()

    test_string=v
    prepare_test_string = v.lower()
    words = re.split(r'(\W)', prepare_test_string)
    words = [word for word in words if word != '']

    for i in range(len(words)):
        words[i] = stemmer.stem_word(words[i])

    stem_test_string = ''.join(words)
    #print('Source: %s\nStemmed: %s' % (test_string, stem_test_string))

    data_xx.append(stem_test_string)


data_x=np.array(data_xx)

data_y = data['target_numeric']

kf = ShuffleSplit(n_splits=1, test_size=0.3, random_state=0)

for train_index, test_index in kf.split(data_x,data_y):
    x_train, x_test = data_x[train_index], data_x[test_index]
    y_train, y_test = data_y.iloc[train_index], data_y.iloc[test_index]

    x_train_vectorized = vectorizer.fit_transform(x_train)
    x_test_vectorized = vectorizer.transform(x_test)
    a=x_train_vectorized[1].toarray()
    print(a)
    print(len(a[0]))
    print(a[0])
    print((a[0][0]))

    clf = SVC(kernel='linear', probability=True, cache_size=200, max_iter=20000)
    clf.fit(x_train_vectorized, y_train)
    y_pred = clf.predict(x_test_vectorized)

    score_accuracy = accuracy_score(y_test, y_pred)
    score_f1 = sklearn.metrics.f1_score(y_test, y_pred, average='macro')
    score_precision = precision_score(y_test, y_pred, average='macro')
    score_recall = recall_score(y_test, y_pred, average='macro')

    a1.append(score_accuracy)
    f1.append(score_f1)
    print(y_pred)
    print('F1 Score = ', score_f1)
    print('Accuracy Score = ', score_accuracy)
    print('Precision Score = ', score_precision)
    print('Recall Score = ', score_recall)


print("acc",np.average(a1))
print("f1",np.average(f1))

#data = pd.read_csv("https://docs.google.com/spreadsheets/d/1FQBeZxBKBLgfT5VHUZBYGK2ht4O4tmQ9Qt3zbcjfYVU/gviz/tq?tqx=out:csv&sheet=Dovzhenko_Yanocskiy")
data = pd.read_csv("train.csv")


a1 = []
f1 = []
class_names = ['Pravda', 'Politeka']

vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)

data_x = data['text']

print(data['text'])

data_xx=[]
for v in data_x:
    import re
    from uk_stemmer import UkStemmer

    stemmer = UkStemmer()

    test_string=v
    prepare_test_string = v.lower()
    words = re.split(r'(\W)', prepare_test_string)
    words = [word for word in words if word != '']

    for i in range(len(words)):
        words[i] = stemmer.stem_word(words[i])

    stem_test_string = ''.join(words)
    #print('Source: %s\nStemmed: %s' % (test_string, stem_test_string))

    data_xx.append(stem_test_string)


data_x=np.array(data_xx)

data_y = data['target']

kf = ShuffleSplit(n_splits=2, test_size=0.3, random_state=0)

for train_index, test_index in kf.split(data_x,data_y):
    x_train, x_test = data_x[train_index], data_x[test_index]
    y_train, y_test = data_y[train_index], data_y[test_index]

    x_train_vectorized = vectorizer.fit_transform(x_train)
    x_test_vectorized = vectorizer.transform(x_test)

    clf = SVR()
    clf.fit(x_train_vectorized, y_train)
    y_pred = clf.predict(x_test_vectorized)

    score_accuracy = accuracy_score(y_test, np.rint(y_pred))
    score_f1 = sklearn.metrics.f1_score(y_test, np.rint(y_pred), average='macro')
    score_precision = precision_score(y_test, np.rint(y_pred), average='macro')
    score_recall = recall_score(y_test, np.rint(y_pred), average='macro')

    a1.append(score_accuracy)
    f1.append(score_f1)

    print('F1 Score = ', score_f1)
    print('Accuracy Score = ', score_accuracy)
    print('Precision Score = ', score_precision)
    print('Recall Score = ', score_recall)


print("acc",np.average(a1))
print("f1",np.average(f1))

# Commented out IPython magic to ensure Python compatibility.

#data = pd.read_csv("https://docs.google.com/spreadsheets/d/1TSpzR1juNcdcL_08TbUvKTpsfdMQPohdhVqO5rmPGEY/gviz/tq?tqx=out:csv&sheet=Mirniy_Vovchok")
data = pd.read_csv("train.csv")


a1 = []
f1 = []
class_names = ['Pravda', 'Politeka']

vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, min_df=0.01, max_df=0.95)
#data['text'][2636] = """"Послухайте мене, росіяни! Ну чи як ви там себе називаєте? Імперці, московити, вірні путінці або віддані кремлівці. Мені власне все одно. Давайте зупинимося на звичному – росіяни.Далі текст мовою оригіналуВы хотя бы на секунду представляете себе уровень не любви к вам, со стороны украинцев? Нет, не ко всем "братским" соседям. Я часто вижу в Киеве, перепуганные группы российских туристов, которые словно цыплята жмутся к экскурсоводу. Наверное они уверенны, что их тут съедят специальные отряды бандеровцев? Вы приехали в Украину с миром? Не вопрос. Здесь Хрещатик, там Подол, смотрите, любуйтесь. За углом рестораны. Мусор доносите до урны. Дорогу переходите на зелёный свет. Не забывайте, что вы в гостях.Відео дняНо если вы вынашиваете планы, пройтись по Украине огнём и мечом, то вы в самом кошмарном сне не можете представить, что вас ждёт. Ведь уровень не любви к вам, (и это очень мягко сказано, чтобы фейсбук не обиделся), запредельный. Он же на генном уровне.Вам вспомнят всё! Каждую каплю крови, пролитую краснопузыми бандитами Муравьева в Киеве. Вспомнят каждую слезу, после десятилетних репрессий НКВД и КГБ. Отдельно, с крайним ожесточением, вам будут вспоминать каждое зернышко отобранное у крестьян и следовательно каждую загубленную украинскую жизнь. Каждого, вы понимаете, каждого вспомнят вам из тех, кто умер от голода в Украине, в тот момент, когда коммунисты наедали себе рожи в три кубометра. И эти воспоминания вам будут возвращать по отдельному прайсу.Вам снова вспомнят, каждую жизнь, которую ваши руки и ваше оружие, забрали с 2014 года. Да, конечно… Вы наверное разнесёте своими хваленными "Калибрами" стену Памяти возле Михайловского золотоверхого. Но ведь каждое имя, каждая фотография в сердце, в генах нации…Вы хотите прийти в Украину с огнём и мечом? Так вы получите здесь, и огонь и меч. А так же свинец и сталь. А главное, вы получите ЯРОСТЬ накопленную веками…Оно вам надо, такую проблему на всю голову?"""

data_x = data['text']

print(data['text'])

data_xx=[]
for v in data_x:
    import re
    from uk_stemmer import UkStemmer

    stemmer = UkStemmer()

    test_string=v
    prepare_test_string = v.lower()
    words = re.split(r'(\W)', prepare_test_string)
    words = [word for word in words if word != '']

    for i in range(len(words)):
        words[i] = stemmer.stem_word(words[i])

    stem_test_string = ''.join(words)
    #print('Source: %s\nStemmed: %s' % (test_string, stem_test_string))

    data_xx.append(stem_test_string)


data_x=np.array(data_xx)

data_y = data['target']

kf = ShuffleSplit(n_splits=2, test_size=0.3, random_state=0)

for train_index, test_index in kf.split(data_x,data_y):
    x_train, x_test = data_x[train_index], data_x[test_index]
    y_train, y_test = data_y[train_index], data_y[test_index]

    x_train_vectorized = vectorizer.fit_transform(x_train)
    x_test_vectorized = vectorizer.transform(x_test)

    clf = SVC(kernel='linear', probability=True, cache_size=200, max_iter=20000)
    clf.fit(x_train_vectorized, y_train)
    y_pred = clf.predict(x_test_vectorized)

    score_accuracy = accuracy_score(y_test, y_pred)
    score_f1 = sklearn.metrics.f1_score(y_test, y_pred, average='macro')
    score_precision = precision_score(y_test, y_pred, average='macro')
    score_recall = recall_score(y_test, y_pred, average='macro')

    a1.append(score_accuracy)
    f1.append(score_f1)

    print('F1 Score = ', score_f1)
    print('Accuracy Score = ', score_accuracy)
    print('Precision Score = ', score_precision)
    print('Recall Score = ', score_recall)

    c = make_pipeline(vectorizer, clf)
    explainer = LimeTextExplainer(class_names=class_names)
    print(test_index)
    idx = 0

    exp = explainer.explain_instance(x_test[idx], c.predict_proba, num_features=400)

    print('Document id: %d' % idx)
    print('Probability(Sciense) =', c.predict_proba([x_test[idx]])[0,1])
    #print('True class: %s' % class_names[y_test[idx]])
    print('Original prediction:', clf.predict_proba(x_test_vectorized[idx])[0,1])

    tmp = x_test_vectorized[idx].copy()

    print('Prediction removing some features:', clf.predict_proba(tmp)[0,1])
    print('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(x_test_vectorized[idx])[0,1])

#     %matplotlib inline
    fig = exp.as_pyplot_figure()
    fig.set_size_inches(18.5, 40)
    exp.save_to_file("result2.html")
    exp.show_in_notebook(text=True)

    exp.as_list()
print("acc",np.average(a1))
print("f1",np.average(f1))